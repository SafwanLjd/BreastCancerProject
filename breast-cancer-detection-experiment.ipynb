{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4891287,"sourceType":"datasetVersion","datasetId":2836274}],"dockerImageVersionId":30580,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Breast Cancer Detection Project","metadata":{}},{"cell_type":"markdown","source":"## 1st Experiment (Using Tensorflow)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nimport os\n\n\nBASE_DIR = \"/kaggle/input/breast-cancer-jpg-image-dataset-of-cbisddsm/k_CBIS-DDSM\"\nCALC_CSV_PATH = f\"{BASE_DIR}/calc_case(with_jpg_img).csv\"\nMASS_CSV_PATH = f\"{BASE_DIR}/mass_case(with_jpg_img).csv\"\nBATCH_SIZE = 64\n\ndef preprocess_image(image, label):\n    image = tf.io.read_file(image) # Load\n    image = tf.image.decode_jpeg(image, channels=1) # Grey Scale\n    image = tf.image.resize(image, [224, 224]) # Scale Down\n    image = image / 255.0 # Normalize\n\n    return image, label\n\n\ndef create_dataset(image_paths, labels, batch_size):\n    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n    \n    return dataset\n\n\ndef create_label(abnormality_type, pathology):\n    if pathology in [\"BENIGN\", \"BENIGN_WITHOUT_CALLBACK\"]:\n        pathology_label = 0  # Benign\n    else:\n        pathology_label = 1  # Malignant\n\n    if abnormality_type == \"MASS\":\n        return pathology_label  # 0: Benign Mass; 1: Malignant Mass\n\n    else:\n        return 2 + pathology_label  # 2: Benign Calcification; 3: Malignant Calcification\n\n\n\ncalc_case_df = pd.read_csv(CALC_CSV_PATH)\nmass_case_df = pd.read_csv(MASS_CSV_PATH)\n\nimage_paths = []\nlabels = []\n\nfor _, row in calc_case_df.iterrows():\n    image_path = os.path.join(BASE_DIR, row[\"jpg_fullMammo_img_path\"])\n    label = create_label(row[\"abnormality type\"], row[\"pathology\"])\n    image_paths.append(image_path)\n    labels.append(label)\n\nfor _, row in mass_case_df.iterrows():\n    image_path = os.path.join(BASE_DIR, row[\"jpg_fullMammo_img_path\"])\n    label = create_label(row[\"abnormality type\"], row[\"pathology\"])\n    image_paths.append(image_path)\n    labels.append(label)\n\n\ntrain_image_paths, train_labels, test_image_paths, test_labels = [], [], [], []\n\nfor image_path, label in zip(image_paths, labels):\n    if \"Train\" in image_path:\n        train_image_paths.append(image_path)\n        train_labels.append(label)\n\n    elif \"Test\" in image_path:\n        test_image_paths.append(image_path)\n        test_labels.append(label)\n\n# Create data generators for training and testing with data augmentation\ntrain_datagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='nearest'\n)\n\n\ntest_datagen = ImageDataGenerator()\n\n# Apply data augmentation to the training dataset\ntrain_augmented_dataset = create_dataset(train_image_paths, train_labels, BATCH_SIZE)\ntrain_augmented_iterator = tf.data.Dataset.as_numpy_iterator(train_augmented_dataset)\ntrain_augmented_images, train_augmented_labels = next(train_augmented_iterator)\ntrain_augmented_dataset = train_datagen.flow(train_augmented_images, train_augmented_labels)\n\n# Apply data augmentation to the test dataset (without random transformations)\ntest_augmented_dataset = create_dataset(test_image_paths, test_labels, BATCH_SIZE)\ntest_augmented_iterator = tf.data.Dataset.as_numpy_iterator(test_augmented_dataset)\ntest_augmented_images, test_augmented_labels = next(test_augmented_iterator)\ntest_augmented_dataset = test_datagen.flow(test_augmented_images, test_augmented_labels)\n\n\n# Define the CNN model\nmodel = models.Sequential()\n\n# Convolutional layer with 64 filters, a 3x3 kernel, 'relu' activation, and batch normalization\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 1)))\nmodel.add(layers.BatchNormalization())\n# Max pooling layer with a 2x2 pool size\nmodel.add(layers.MaxPooling2D((2, 2)))\n\n# Another convolutional layer with 128 filters, a 3x3 kernel, 'relu' activation, and batch normalization\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))\nmodel.add(layers.BatchNormalization())\n# Another max pooling layer\nmodel.add(layers.MaxPooling2D((2, 2)))\n\n# Another convolutional layer with 256 filters, a 3x3 kernel, 'relu' activation, and batch normalization\nmodel.add(layers.Conv2D(256, (3, 3), activation='relu'))\nmodel.add(layers.BatchNormalization())\n# Another max pooling layer\nmodel.add(layers.MaxPooling2D((2, 2)))\n\n# Flatten layer to convert the 2D output to a vector\nmodel.add(layers.Flatten())\n\n# Fully connected layer with 512 neurons, 'relu' activation, and dropout\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dropout(0.2))  # Adjust the dropout rate as needed\n\n# Output layer with four neurons (for the four classes) and 'softmax' activation\nmodel.add(layers.Dense(4, activation='softmax'))\n\n\n# Compile the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Display the model summary\nmodel.summary()\n\n# Define callbacks\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n\n# Train the model\nepochs = 20\n\n# Train the model with data augmentation\nhistory = model.fit(\n    train_augmented_dataset,\n    epochs=epochs,\n    validation_data=test_augmented_dataset,\n    callbacks=[early_stopping, model_checkpoint])\n\n# Evaluate the model on the original test set\ntest_loss, test_accuracy = model.evaluate(test_augmented_dataset)\nprint(f\"\\nTest Accuracy: {test_accuracy * 100:.2f}%\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2nd Experiment (Using Pytorch Lightning)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport os\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchvision import transforms\nfrom torchvision.io import read_image\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.trainer import Trainer\nfrom pytorch_lightning.callbacks import EarlyStopping, GradientAccumulationScheduler, ModelCheckpoint\nfrom pytorch_lightning.accelerators import CUDAAccelerator\n\nBASE_DIR = \"/kaggle/input/breast-cancer-jpg-image-dataset-of-cbisddsm/k_CBIS-DDSM\"\nCALC_CSV_PATH = f\"{BASE_DIR}/calc_case(with_jpg_img).csv\"\nMASS_CSV_PATH = f\"{BASE_DIR}/mass_case(with_jpg_img).csv\"\nBATCH_SIZE = 64\n\n# Instantiate the model\nclass Net(pl.LightningModule):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n        self.batchnorm1 = nn.BatchNorm2d(64)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.batchnorm2 = nn.BatchNorm2d(128)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.batchnorm3 = nn.BatchNorm2d(256)\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.flatten = nn.Flatten()\n        self.fc1 = nn.Linear(256 * 28 * 28, 512)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(512, 4)  # Assuming 4 classes for the output layer\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.batchnorm1(x)\n        x = self.relu(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.batchnorm2(x)\n        x = self.relu(x)\n        x = self.pool2(x)\n        x = self.conv3(x)\n        x = self.batchnorm3(x)\n        x = self.relu(x)\n        x = self.pool3(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n    def training_step(self, batch, batch_idx):\n        images, labels = batch\n        outputs = self(images)\n        loss = F.cross_entropy(outputs, labels)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        images, labels = batch\n        outputs = self(images)\n        loss = F.cross_entropy(outputs, labels)\n\n        # Calculate accuracy and log it\n        _, predicted = torch.max(outputs, 1)\n        accuracy = torch.sum(predicted == labels.data).item() / len(labels)\n        self.log(\"val_accuracy\", accuracy, prog_bar=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        images, labels = batch\n        outputs = self(images)\n        loss = F.cross_entropy(outputs, labels)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=0.001)\n        return optimizer\n\n# Custom dataset class\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        label = self.labels[idx]\n\n        # Use PIL.Image.open to read the image\n        image = Image.open(img_path).convert('L')  # 'L' mode for grayscale\n        if self.transform:\n            image = self.transform(image)\n\n        # Move image and label to GPU\n        return image, label\n\n# Image transformations with data augmentation, dropout, and normalization\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),  # Add vertical flip\n    transforms.RandomRotation(15),  # Rotate randomly up to 15 degrees\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Adjust color\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5]),  # Adjust mean and std as needed\n    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3)),  # Optional: Random Erasing\n])\n\ndef create_label(abnormality_type, pathology):\n    if pathology in [\"BENIGN\", \"BENIGN_WITHOUT_CALLBACK\"]:\n        pathology_label = 0  # Benign\n    else:\n        pathology_label = 1  # Malignant\n\n    if abnormality_type == \"MASS\":\n        return pathology_label  # 0: Benign Mass; 1: Malignant Mass\n\n    else:\n        return 2 + pathology_label  # 2: Benign Calcification; 3: Malignant Calcification\n    \n# Load and preprocess the dataset\ncalc_case_df = pd.read_csv(CALC_CSV_PATH)\nmass_case_df = pd.read_csv(MASS_CSV_PATH)\n\nimage_paths = []\nlabels = []\n\nfor _, row in calc_case_df.iterrows():\n    image_path = os.path.join(BASE_DIR, row[\"jpg_fullMammo_img_path\"])\n    label = create_label(row[\"abnormality type\"], row[\"pathology\"])\n    image_paths.append(image_path)\n    labels.append(label)\n\nfor _, row in mass_case_df.iterrows():\n    image_path = os.path.join(BASE_DIR, row[\"jpg_fullMammo_img_path\"])\n    label = create_label(row[\"abnormality type\"], row[\"pathology\"])\n    image_paths.append(image_path)\n    labels.append(label)\n\n# Split the data into training and testing sets\ntrain_image_paths, test_image_paths, train_labels, test_labels = train_test_split(\n    image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n)\n\n# Create data loaders for training and testing with data augmentation\ntrain_dataset = CustomDataset(train_image_paths, train_labels, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_dataset = CustomDataset(test_image_paths, test_labels, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Instantiate the model, loss function, and optimizer\nmodel = Net()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# Define PyTorch Lightning Trainer\ntrainer = pl.Trainer(\n    max_epochs=20,\n    #accelerator=CUDAAccelerator(),  # Use CUDAAccelerator\n    callbacks=[\n        #EarlyStopping(monitor='val_accuracy', patience=3, mode='max'),  # Early stopping with accuracy\n        GradientAccumulationScheduler(scheduling={4: 2}),  # Gradient accumulation\n        ModelCheckpoint(\n            dirpath=\"./saved_models\",\n            filename=\"best_model\",\n            save_top_k=1,\n            monitor='val_accuracy', \n            mode='max',\n        )\n    ],\n)\n\n# Train the model using PyTorch Lightning Trainer\ntrainer.fit(model, train_loader)\n\n# Test the model\ntrainer.test(dataloaders=test_loader)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:11:10.378452Z","iopub.execute_input":"2023-11-15T13:11:10.378919Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:74: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:293: The number of training batches (45) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39b1b651bd8c4ed0b7d864b9070f5bdf"}},"metadata":{}}]}]}
